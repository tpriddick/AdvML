{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8c5ded",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Week 4 Notes\n",
    "### (We skipped week 3)\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "Taking a series of phrases, and tokenizing each individual word, then outputting a list of the tokenized words. From there, each list is assigned a 3-dimensional tensor. Essentially, we are taking each observation in a data set, where each observation is a phrase, and turning them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6868ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b62879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffd4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41af650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\anaconda3\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63d5006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler\\anaconda3\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "C:\\Users\\tyler\\anaconda3\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06e4993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1b381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "X_news, y_news = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Preprocess text using CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "# Stop words are the words that are so commonly used that they can be ignored and are meaningless\n",
    "# These would be words like \"a\", \"is\", \"the\", and others\n",
    "X_news_vec = vectorizer.fit_transform(X_news).toarray()\n",
    "\n",
    "# Train-test split\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news_vec, y_news, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25926d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_news_tensor = torch.tensor(X_train_news, dtype=torch.float32)\n",
    "y_train_news_tensor = torch.tensor(y_train_news, dtype=torch.long)\n",
    "X_test_news_tensor = torch.tensor(X_test_news, dtype=torch.float32)\n",
    "y_test_news_tensor = torch.tensor(y_test_news, dtype=torch.long)\n",
    "\n",
    "# DataLoader for 20 Newsgroups dataset\n",
    "train_dataset_news = TensorDataset(X_train_news_tensor, y_train_news_tensor)\n",
    "test_dataset_news = TensorDataset(X_test_news_tensor, y_test_news_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader_news = DataLoader(train_dataset_news, batch_size=batch_size, shuffle=True)\n",
    "test_loader_news = DataLoader(test_dataset_news, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "014b089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class NLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU() # activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and define hyperparameters\n",
    "input_dim_news = X_news_vec.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim_news = len(newsgroups.target_names)  # Number of classes in 20 Newsgroups dataset\n",
    "\n",
    "model_news = NLPClassifier(input_dim_news, hidden_dim, output_dim_news)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer_news = optim.Adam(model_news.parameters())\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca6c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.6590041987976785\n",
      "Epoch 2/5, Loss: 0.80308229665635\n",
      "Epoch 3/5, Loss: 0.611717043286663\n",
      "Epoch 4/5, Loss: 0.49422539650636205\n",
      "Epoch 5/5, Loss: 0.42705816478799963\n"
     ]
    }
   ],
   "source": [
    "# Training loop for 20 Newsgroups dataset\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Train the 20 Newsgroups model\n",
    "train_model(model_news, train_loader_news, optimizer_news, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cfa35f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7636604774535809\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_news, test_loader_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78ed64",
   "metadata": {},
   "source": [
    "### With embeddings\n",
    "\n",
    "Recall that embedding is the processing of converting the tokenized arrays into the three-dimensional tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f509fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize and encode the data using torchtext\n",
    "train_data_tokens = [tokenizer(text) for text in X_train]\n",
    "test_data_tokens = [tokenizer(text) for text in X_test]\n",
    "\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab_from_iterator(train_data_tokens, specials=[\"<unk>\"])  # Handle OOV tokens with \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7d53283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '(',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " ')',\n",
       " 'subject',\n",
       " 're',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " ',',\n",
       " 'or',\n",
       " 'is',\n",
       " 'this',\n",
       " 'newsgroup',\n",
       " 'dead',\n",
       " '?',\n",
       " 'organization',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'lines',\n",
       " '24',\n",
       " 'nntp-posting-host',\n",
       " 'enterpoop',\n",
       " '.',\n",
       " 'mit',\n",
       " '.',\n",
       " 'edu',\n",
       " 'to',\n",
       " 'xpert@expo',\n",
       " '.',\n",
       " 'lcs',\n",
       " '.',\n",
       " 'mit',\n",
       " '.',\n",
       " 'edu',\n",
       " ',',\n",
       " 'rlm@helen',\n",
       " '.',\n",
       " 'surfcty',\n",
       " '.',\n",
       " 'com',\n",
       " '#',\n",
       " '#',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'gotten',\n",
       " 'very',\n",
       " 'few',\n",
       " 'posts',\n",
       " 'on',\n",
       " 'this',\n",
       " 'group',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'couple',\n",
       " 'days',\n",
       " '.',\n",
       " '(',\n",
       " 'i',\n",
       " '#',\n",
       " 'recently',\n",
       " 'added',\n",
       " 'it',\n",
       " 'to',\n",
       " 'my',\n",
       " 'feed',\n",
       " 'list',\n",
       " '.',\n",
       " ')',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " ',',\n",
       " 'or',\n",
       " 'is',\n",
       " 'this',\n",
       " 'group',\n",
       " '#',\n",
       " 'near',\n",
       " 'death',\n",
       " '?',\n",
       " '#',\n",
       " 'seen',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'side',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'getting',\n",
       " 'about',\n",
       " 'the',\n",
       " 'right',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'traffic',\n",
       " '.',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " '---',\n",
       " 'tgv',\n",
       " 'window',\n",
       " 'washer',\n",
       " '-------------------------------',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '---------',\n",
       " 'waking',\n",
       " 'a',\n",
       " 'person',\n",
       " 'unnecessarily',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'considered',\n",
       " '-',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'a',\n",
       " 'capital',\n",
       " 'crime',\n",
       " '.',\n",
       " 'for',\n",
       " 'a',\n",
       " 'first',\n",
       " 'offense',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'from',\n",
       " 'the',\n",
       " 'notebooks',\n",
       " 'of',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " '---',\n",
       " 'tgv',\n",
       " 'window',\n",
       " 'washer',\n",
       " '-------------------------------',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '---------',\n",
       " 'waking',\n",
       " 'a',\n",
       " 'person',\n",
       " 'unnecessarily',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'considered',\n",
       " '-',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'a',\n",
       " 'capital',\n",
       " 'crime',\n",
       " '.',\n",
       " 'for',\n",
       " 'a',\n",
       " 'first',\n",
       " 'offense',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'from',\n",
       " 'the',\n",
       " 'notebooks',\n",
       " 'of',\n",
       " 'lazarus',\n",
       " 'long']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens[0] # lists all of the tokens of the first email in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b29c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: mahan@TGV.COM (Patrick L. Mahan)\\nSubject: Re: Is it just me, or is this newsgroup dead?\\nOrganization: The Internet\\nLines: 24\\nNNTP-Posting-Host: enterpoop.mit.edu\\nTo: xpert@expo.lcs.mit.edu, rlm@helen.surfcty.com\\n\\n#\\n# I've gotten very few posts on this group in the last couple days.  (I\\n# recently added it to my feed list.)  Is it just me, or is this group\\n# near death?\\n#\\n\\nSeen from the mailing list side, I'm getting about the right amount of\\ntraffic.\\n\\nPatrick L. Mahan\\n\\n--- TGV Window Washer ------------------------------- Mahan@TGV.COM ---------\\n\\nWaking a person unnecessarily should not be considered  - Lazarus Long\\na capital crime.  For a first offense, that is            From the Notebooks of\\n\\t\\t\\t\\t\\t\\t\\t  Lazarus Long\\n\\nPatrick L. Mahan\\n\\n--- TGV Window Washer ------------------------------- Mahan@TGV.COM ---------\\n\\nWaking a person unnecessarily should not be considered  - Lazarus Long\\na capital crime.  For a first offense, that is            From the Notebooks of\\n\\t\\t\\t\\t\\t\\t\\t  Lazarus Long\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4adf3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "max_seq_length = 100\n",
    "train_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in train_data_tokens]\n",
    "test_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in test_data_tokens]\n",
    "\n",
    "X_train_tensor = pad_sequence(train_data_padded, batch_first=True, padding_value=0)\n",
    "X_test_tensor = pad_sequence(test_data_padded, batch_first=True, padding_value=0)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4e3be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   22, 13640,     1,    47,     8,  2323,   367,     1, 10692,     7,\n",
       "           39,    46,    13,    16,    84,    70,     2,    34,    13,    25,\n",
       "          985,   684,    15,    41,     3,   378,    40,   368,    93,  5570,\n",
       "            1,   418,     1,    30,     5,  5549,     1,  1862,     1,   418,\n",
       "            1,    30,     2, 33149,     1, 29497,     1,    47,   194,   194,\n",
       "           11,     4,   139,  2104,   129,   250,  1534,    23,    25,   275,\n",
       "           12,     3,   198,   652,   516,     1,     8,    11,   194,   881,\n",
       "         1695,    16,     5,    51,  3660,   305,     1,     7,    13,    16,\n",
       "           84,    70,     2,    34,    13,    25,   275,   194,   941,   625,\n",
       "           15,   194,   360,    22,     3,  1495,   305,   612,     2,    11])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22324eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(22) # this should be the first word of the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0ae164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   22, 85556,     1,   962,     1,    30,     8,  1319, 85555,     7,\n",
       "           39,    46,  9287,  3200,  1272,    41,   609,   508,     6,   354,\n",
       "            2,  2782,    40,   304,    93, 13249,     1,   962,     1,    30,\n",
       "        28659,     1, 10472,     1,    30,     8,  3689, 29261,     7,    62,\n",
       "         3440,   198,  9287,  3200,   195,    14,    65,   370, 15318,    70,\n",
       "           10,  2223,     4,    32, 23123,   100,  6184,     1,    11,   175,\n",
       "            3,  1272,   105,   107,  2690,   713,    31,    25, 17137,  6523,\n",
       "            2,    36,     3,  9287,  3200,    13,  2690,  2460,     2,    44,\n",
       "           50,   201,    42,     9,  5921, 40809,   914,   101,  1106, 11377,\n",
       "         1852,     6,   156, 13039,   188,    18, 20192,   164, 49874,    15])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor[0] # the first entry in our test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5889bc0",
   "metadata": {},
   "source": [
    "Note above that a token of 0 in our test data indicates that that word did not appear in our training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86c686cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupsDataset(Dataset):\n",
    "    def __init__(self, data_tokens, targets, vocab, max_seq_length=100):\n",
    "        self.data_tokens = data_tokens\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data_tokens[idx][:self.max_seq_length]\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"] for token in tokens]\n",
    "        return torch.tensor(token_ids), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "122f35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom Dataset for training and testing\n",
    "train_dataset = NewsGroupsDataset(train_data_tokens, y_train, vocab, max_seq_length)\n",
    "test_dataset = NewsGroupsDataset(test_data_tokens, y_test, vocab, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451379be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   22, 13640,     1,    47,     8,  2323,   367,     1, 10692,     7,\n",
       "            39,    46,    13,    16,    84,    70,     2,    34,    13,    25,\n",
       "           985,   684,    15,    41,     3,   378,    40,   368,    93,  5570,\n",
       "             1,   418,     1,    30,     5,  5549,     1,  1862,     1,   418,\n",
       "             1,    30,     2, 33149,     1, 29497,     1,    47,   194,   194,\n",
       "            11,     4,   139,  2104,   129,   250,  1534,    23,    25,   275,\n",
       "            12,     3,   198,   652,   516,     1,     8,    11,   194,   881,\n",
       "          1695,    16,     5,    51,  3660,   305,     1,     7,    13,    16,\n",
       "            84,    70,     2,    34,    13,    25,   275,   194,   941,   625,\n",
       "            15,   194,   360,    22,     3,  1495,   305,   612,     2,    11]),\n",
       " tensor(5, dtype=torch.int32))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0) # again, the first email represented as tokens\n",
    "# this falls into class 5, whatever that happens to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a82aa09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac8f73b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network with an embedding layer\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded_avg = torch.mean(embedded, dim=1)\n",
    "        x = self.fc1(embedded_avg)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a75f65a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_news.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1583fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 500\n",
    "hidden_dim = 128\n",
    "#output_dim = len(label_encoder.classes_)\n",
    "output_dim = y_train_news.max() + 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1f68516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ddb133d210>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8c1f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1186\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1187\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e14831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e534d596",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1186\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1187\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "#device = torch.device('mps')\n",
    "#model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666375b0",
   "metadata": {},
   "source": [
    "## IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e399a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e467b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the IMDB dataset (donwnload from canvas)\n",
    "imdb_data = pd.read_csv('./IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3c3f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea02ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imdb = imdb_data['review'].values\n",
    "y_imdb = imdb_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f82458e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imdb, y_imdb, test_size=0.7, random_state=42)\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize and encode the data using torchtext\n",
    "train_data_tokens = [tokenizer(text) for text in X_train]\n",
    "test_data_tokens = [tokenizer(text) for text in X_test]\n",
    "\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab_from_iterator(train_data_tokens, specials=[\"<unk>\"])  # Handle OOV tokens with \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfb84ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'fiendish',\n",
       " 'plot',\n",
       " 'of',\n",
       " 'dr',\n",
       " '.',\n",
       " 'fu',\n",
       " 'manchu',\n",
       " '(',\n",
       " '1980',\n",
       " ')',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'hands',\n",
       " 'down',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'film',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'ever',\n",
       " 'seen',\n",
       " '.',\n",
       " 'what',\n",
       " 'a',\n",
       " 'sad',\n",
       " 'way',\n",
       " 'for',\n",
       " 'a',\n",
       " 'great',\n",
       " 'comedian',\n",
       " 'to',\n",
       " 'go',\n",
       " 'out',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8064cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "train_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in train_data_tokens]\n",
    "test_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in test_data_tokens]\n",
    "\n",
    "X_train_tensor = pad_sequence(train_data_padded, batch_first=True, padding_value=0)\n",
    "X_test_tensor = pad_sequence(test_data_padded, batch_first=True, padding_value=0)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47e0e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 24242,   124,     6,   765,     2,  2191, 15577,    25,  2554,\n",
       "           23,     2,    13,     9,   940,   195,     1,   241,    22,    12,\n",
       "            8,   143,   128,   113,     2,    54,     5,   658,   104,    19,\n",
       "            5,    83,  2618,     7,   152,    51,     2,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa7a5362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fiendish'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(24242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d3b825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_tokens, targets, vocab, max_seq_length=100):\n",
    "        self.data_tokens = data_tokens\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data_tokens[idx][:self.max_seq_length]\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk\"] for token in tokens]\n",
    "        return torch.tensor(token_ids), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "733c3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom Dataset for training and testing\n",
    "train_dataset = IMDBDataset(train_data_tokens, y_train, vocab, max_seq_length)\n",
    "test_dataset = IMDBDataset(test_data_tokens, y_test, vocab, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db5a19b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1, 24242,   124,     6,   765,     2,  2191, 15577,    25,  2554,\n",
       "            23,     2,    13,     9,   940,   195,     1,   241,    22,    12,\n",
       "             8,   143,   128,   113,     2,    54,     5,   658,   104,    19,\n",
       "             5,    83,  2618,     7,   152,    51,     2]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c3794b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a7ace1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded_avg = torch.mean(embedded, dim=1)\n",
    "        x = self.fc1(embedded_avg)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "421d4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 500\n",
    "hidden_dim = 128\n",
    "#output_dim = len(label_encoder.classes_)\n",
    "output_dim = y_train.max() + 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = FFNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e667200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.43851688504219055\n",
      "Epoch 2/10, Loss: 0.4423471689224243\n",
      "Epoch 3/10, Loss: 0.3188078701496124\n",
      "Epoch 4/10, Loss: 0.43826207518577576\n",
      "Epoch 5/10, Loss: 0.3963536024093628\n",
      "Epoch 6/10, Loss: 0.5216236710548401\n",
      "Epoch 7/10, Loss: 0.35494187474250793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m----> 9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4abf0e79",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7000000000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_test_tensor)\n\u001b[0;32m      5\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_tensor\u001b[38;5;241m.\u001b[39mnumpy(), predicted\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m, in \u001b[0;36mFFNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     12\u001b[0m     embedded_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(embedded, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(embedded_avg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7000000000 bytes."
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725d15a",
   "metadata": {},
   "source": [
    "## Pretrained Natural Language Processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
