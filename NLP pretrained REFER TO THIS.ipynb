{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfe83aa9",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"./images/text_embedding.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f021dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "X_news, y_news = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Preprocess text using CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X_news_vec = vectorizer.fit_transform(X_news).toarray()\n",
    "\n",
    "# Train-test split\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news_vec, y_news, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0533be",
   "metadata": {},
   "source": [
    "### With embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd52a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenize and encode the data using torchtext\n",
    "train_data_tokens = [tokenizer(text) for text in X_train]\n",
    "test_data_tokens = [tokenizer(text) for text in X_test]\n",
    "\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab_from_iterator(train_data_tokens, specials=[\"<unk>\"])  # Handle OOV tokens with \"<unk>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bd9d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '(',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " ')',\n",
       " 'subject',\n",
       " 're',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " ',',\n",
       " 'or',\n",
       " 'is',\n",
       " 'this',\n",
       " 'newsgroup',\n",
       " 'dead',\n",
       " '?',\n",
       " 'organization',\n",
       " 'the',\n",
       " 'internet',\n",
       " 'lines',\n",
       " '24',\n",
       " 'nntp-posting-host',\n",
       " 'enterpoop',\n",
       " '.',\n",
       " 'mit',\n",
       " '.',\n",
       " 'edu',\n",
       " 'to',\n",
       " 'xpert@expo',\n",
       " '.',\n",
       " 'lcs',\n",
       " '.',\n",
       " 'mit',\n",
       " '.',\n",
       " 'edu',\n",
       " ',',\n",
       " 'rlm@helen',\n",
       " '.',\n",
       " 'surfcty',\n",
       " '.',\n",
       " 'com',\n",
       " '#',\n",
       " '#',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'gotten',\n",
       " 'very',\n",
       " 'few',\n",
       " 'posts',\n",
       " 'on',\n",
       " 'this',\n",
       " 'group',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'couple',\n",
       " 'days',\n",
       " '.',\n",
       " '(',\n",
       " 'i',\n",
       " '#',\n",
       " 'recently',\n",
       " 'added',\n",
       " 'it',\n",
       " 'to',\n",
       " 'my',\n",
       " 'feed',\n",
       " 'list',\n",
       " '.',\n",
       " ')',\n",
       " 'is',\n",
       " 'it',\n",
       " 'just',\n",
       " 'me',\n",
       " ',',\n",
       " 'or',\n",
       " 'is',\n",
       " 'this',\n",
       " 'group',\n",
       " '#',\n",
       " 'near',\n",
       " 'death',\n",
       " '?',\n",
       " '#',\n",
       " 'seen',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mailing',\n",
       " 'list',\n",
       " 'side',\n",
       " ',',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'getting',\n",
       " 'about',\n",
       " 'the',\n",
       " 'right',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'traffic',\n",
       " '.',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " '---',\n",
       " 'tgv',\n",
       " 'window',\n",
       " 'washer',\n",
       " '-------------------------------',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '---------',\n",
       " 'waking',\n",
       " 'a',\n",
       " 'person',\n",
       " 'unnecessarily',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'considered',\n",
       " '-',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'a',\n",
       " 'capital',\n",
       " 'crime',\n",
       " '.',\n",
       " 'for',\n",
       " 'a',\n",
       " 'first',\n",
       " 'offense',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'from',\n",
       " 'the',\n",
       " 'notebooks',\n",
       " 'of',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'patrick',\n",
       " 'l',\n",
       " '.',\n",
       " 'mahan',\n",
       " '---',\n",
       " 'tgv',\n",
       " 'window',\n",
       " 'washer',\n",
       " '-------------------------------',\n",
       " 'mahan@tgv',\n",
       " '.',\n",
       " 'com',\n",
       " '---------',\n",
       " 'waking',\n",
       " 'a',\n",
       " 'person',\n",
       " 'unnecessarily',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'considered',\n",
       " '-',\n",
       " 'lazarus',\n",
       " 'long',\n",
       " 'a',\n",
       " 'capital',\n",
       " 'crime',\n",
       " '.',\n",
       " 'for',\n",
       " 'a',\n",
       " 'first',\n",
       " 'offense',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " 'from',\n",
       " 'the',\n",
       " 'notebooks',\n",
       " 'of',\n",
       " 'lazarus',\n",
       " 'long']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dda95b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: mahan@TGV.COM (Patrick L. Mahan)\\nSubject: Re: Is it just me, or is this newsgroup dead?\\nOrganization: The Internet\\nLines: 24\\nNNTP-Posting-Host: enterpoop.mit.edu\\nTo: xpert@expo.lcs.mit.edu, rlm@helen.surfcty.com\\n\\n#\\n# I've gotten very few posts on this group in the last couple days.  (I\\n# recently added it to my feed list.)  Is it just me, or is this group\\n# near death?\\n#\\n\\nSeen from the mailing list side, I'm getting about the right amount of\\ntraffic.\\n\\nPatrick L. Mahan\\n\\n--- TGV Window Washer ------------------------------- Mahan@TGV.COM ---------\\n\\nWaking a person unnecessarily should not be considered  - Lazarus Long\\na capital crime.  For a first offense, that is            From the Notebooks of\\n\\t\\t\\t\\t\\t\\t\\t  Lazarus Long\\n\\nPatrick L. Mahan\\n\\n--- TGV Window Washer ------------------------------- Mahan@TGV.COM ---------\\n\\nWaking a person unnecessarily should not be considered  - Lazarus Long\\na capital crime.  For a first offense, that is            From the Notebooks of\\n\\t\\t\\t\\t\\t\\t\\t  Lazarus Long\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0] # email one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692ec7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_seq_length = 100\n",
    "train_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in train_data_tokens]\n",
    "test_data_padded = [torch.tensor([vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokens[:max_seq_length]]) for tokens in test_data_tokens]\n",
    "\n",
    "X_train_tensor = pad_sequence(train_data_padded, batch_first=True, padding_value=0)\n",
    "X_test_tensor = pad_sequence(test_data_padded, batch_first=True, padding_value=0)\n",
    "\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fd989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f18ae9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   22, 13640,     1,    47,     8,  2323,   367,     1, 10692,     7,\n",
       "           39,    46,    13,    16,    84,    70,     2,    34,    13,    25,\n",
       "          985,   684,    15,    41,     3,   378,    40,   368,    93,  5570,\n",
       "            1,   418,     1,    30,     5,  5549,     1,  1862,     1,   418,\n",
       "            1,    30,     2, 33149,     1, 29497,     1,    47,   194,   194,\n",
       "           11,     4,   139,  2104,   129,   250,  1534,    23,    25,   275,\n",
       "           12,     3,   198,   652,   516,     1,     8,    11,   194,   881,\n",
       "         1695,    16,     5,    51,  3660,   305,     1,     7,    13,    16,\n",
       "           84,    70,     2,    34,    13,    25,   275,   194,   941,   625,\n",
       "           15,   194,   360,    22,     3,  1495,   305,   612,     2,    11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef2d0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bd7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupsDataset(Dataset):\n",
    "    def __init__(self, data_tokens, targets, vocab, max_seq_length=100):\n",
    "        self.data_tokens = data_tokens\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data_tokens[idx][:self.max_seq_length]\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"] for token in tokens]\n",
    "        return torch.tensor(token_ids), torch.tensor(self.targets[idx])\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70dfd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the custom Dataset for training and testing\n",
    "train_dataset = NewsGroupsDataset(train_data_tokens, y_train, vocab, max_seq_length)\n",
    "test_dataset = NewsGroupsDataset(test_data_tokens, y_test, vocab, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae63b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   22, 13640,     1,    47,     8,  2323,   367,     1, 10692,     7,\n",
       "            39,    46,    13,    16,    84,    70,     2,    34,    13,    25,\n",
       "           985,   684,    15,    41,     3,   378,    40,   368,    93,  5570,\n",
       "             1,   418,     1,    30,     5,  5549,     1,  1862,     1,   418,\n",
       "             1,    30,     2, 33149,     1, 29497,     1,    47,   194,   194,\n",
       "            11,     4,   139,  2104,   129,   250,  1534,    23,    25,   275,\n",
       "            12,     3,   198,   652,   516,     1,     8,    11,   194,   881,\n",
       "          1695,    16,     5,    51,  3660,   305,     1,     7,    13,    16,\n",
       "            84,    70,     2,    34,    13,    25,   275,   194,   941,   625,\n",
       "            15,   194,   360,    22,     3,  1495,   305,   612,     2,    11]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8385b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=lambda batch: (pad_sequence([item[0] for item in batch], batch_first=True, padding_value=0), torch.tensor([item[1] for item in batch])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93952078",
   "metadata": {},
   "source": [
    "# Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce90a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [07:54, 1.82MB/s]                             \n",
      "100%|███████████████████████████████▉| 399999/400000 [00:08<00:00, 45144.97it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove = GloVe(name='6B', dim=50)  # You can choose different dimensions (e.g., 50, 100, 200, 300)\n",
    "\n",
    "# Create an embedding matrix\n",
    "pretrained_embeddings = glove.vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437b2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        vocab_size, embedding_dim = pretrained_embeddings.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded_avg = torch.mean(embedded, dim=1)\n",
    "        x = self.fc1(embedded_avg)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e47caab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_news.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "171ba5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 500\n",
    "hidden_dim = 128\n",
    "#output_dim = len(label_encoder.classes_)\n",
    "output_dim = y_train_news.max() + 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "# Example usage\n",
    "\n",
    "model = FFNN(pretrained_embeddings, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb8dac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.15729960799217224\n",
      "Epoch 2/10, Loss: 0.8579656481742859\n",
      "Epoch 3/10, Loss: 0.0033414126373827457\n",
      "Epoch 4/10, Loss: 0.0010539244394749403\n",
      "Epoch 5/10, Loss: 0.0019872228149324656\n",
      "Epoch 6/10, Loss: 4.663909930968657e-05\n",
      "Epoch 7/10, Loss: 0.00010102357919095084\n",
      "Epoch 8/10, Loss: 0.003932616673409939\n",
      "Epoch 9/10, Loss: 0.00012886294280178845\n",
      "Epoch 10/10, Loss: 1.2785061699105427e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4defdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6cc8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44c24374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8196286472148541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf17069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f2017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18212dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebf043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ccc47f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3f08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280f8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
