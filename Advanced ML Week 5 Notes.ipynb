{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204b5682",
   "metadata": {},
   "source": [
    "# Advanced ML Week 5 Notes\n",
    "\n",
    "## Ensemble Learning and XGBoost\n",
    "\n",
    "Ensemble learning exists to \n",
    "- reduce overfitting\n",
    "- improve predictive performance\n",
    "\n",
    "Ensemble learning involves training a bunch of base models from the input data, and then aggregating the models in some way to produce a final output model\n",
    "\n",
    "### Bias and Variance Trade-Off\n",
    "\n",
    "The optimal model complexity is one that minimizes both bias and variance (which is usually the one that minimizes the total error)\n",
    "\n",
    "### Base Models (weak learners)\n",
    "\n",
    "Base models are our standard ML models (random forest, decision tree, SVM, KNN) that comprise our ensemble learning methods\n",
    "\n",
    "#### Voting and Averaging\n",
    "- Hard voting: marjority rule\n",
    "- Soft voting: weighted probabilities\n",
    "    - Weighted averaging\n",
    "    \n",
    "### Practical Considerations\n",
    "- Overfitting and underfitting\n",
    "- Model interpretability\n",
    "- Computational efficiency\n",
    "\n",
    "## Bagging (Bootstrap Aggregation)\n",
    "- Creating multiple subsets of data with replacement\n",
    "- Training separate models on each subset\n",
    "- Aggregate the results\n",
    "\n",
    "### Example 1: Random Forest\n",
    "- Ensemble of decision trees\n",
    "- Build each tree on a random subset with a random set of features used to split the trees\n",
    "- Reduced correlation between trees, reduced overfitting\n",
    "- Improved generalization capability\n",
    "- For classification, majority voting\n",
    "- For regression, average predictions from all trees\n",
    "\n",
    "### Example 2: Extra Trees (Extremely randomized trees)\n",
    "- More randomized than random forest\n",
    "- Use randomized thresholds in tree-building process\n",
    "- Improved tree diversity\n",
    "- No bootstrap sampling, no resampling\n",
    "- Random split then best split\n",
    "- Used when faster training time is required and variance is an issue\n",
    "\n",
    "### Example 3: Scikit-learn bagging\n",
    "- Weak learners include not only decision tree but also SVM and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c7edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging example\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bddbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.943 (0.080)\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic classification dataset\n",
    "X, y = make_classification(random_state = 1)\n",
    "\n",
    "# Configure the ensemble model\n",
    "model = BaggingClassifier(n_estimators=50)\n",
    "\n",
    "# Configure the resampling method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate the ensemble on the data using the resampling method\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Report ensemble performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52990d5b",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "- Sequentially training models\n",
    "- Each model corrects errors of the previous one\n",
    "- Adjust weights of misclassified samples\n",
    "\n",
    "### Ada (Adaptive) Boosting\n",
    "- Adaptive boosting method\n",
    "- Focuses on hard-to-classify samples\n",
    "- Weight adjustment mechanism\n",
    "- Weighted majority vote (classification) or weighted sum (regression) of the weak learners\n",
    "- Sensitive to noisy data and dependency on weak learners\n",
    "\n",
    "### Gradient Boosting\n",
    "- Optimize a loss function by fitting the residuals of the previous prediction\n",
    "- Iteratively adds models to reduce error\n",
    "- Regulatizion techniques using Gradient descent optimization\n",
    "- Flexible to use various loss fucntions and weak learners\n",
    "- Typically high accuracy, especially on structured/tabular data\n",
    "#### Disadvantages:\n",
    "- Slower to train\n",
    "- More parameters to tune\n",
    "- More complex to implement and optimize\n",
    "\n",
    "### XG Boosting (Extreme Gradient Boosting)\n",
    "- Better\n",
    "\n",
    "### Comparison\n",
    "**Strategy:**\n",
    "- Ada: sequential with weigth adjustment\n",
    "- Gradient: sequential with gradient descent\n",
    "- XG: optimized gradient boosting\n",
    "\n",
    "**Weak Learners:**\n",
    "- Ada: Decision stumps or shallow trees\n",
    "- Gradient: decision trees (various depths)\n",
    "- XG: decision trees with optimizations\n",
    "\n",
    "**Handling Errors:**\n",
    "- Ada: Adjusts weights of misclassified samples\n",
    "- Gradient: fits residuals of combined predicitons\n",
    "- XG: fits residuals with regularization\n",
    "\n",
    "**Training Speed:**\n",
    "- Ada: Fast\n",
    "- Gradient: Moderate to slow\n",
    "- XG: Fast (due to parallel processing)\n",
    "\n",
    "**Prediction speed:**\n",
    "- Ada: fast\n",
    "- Gradient: moderate\n",
    "- XG: fast\n",
    "\n",
    "**Regularization:**\n",
    "- Ada: no\n",
    "- Gradient: no (basic versions)\n",
    "- XG: Yes (L1 and L2 regularization)\n",
    "\n",
    "**Complexity:**\n",
    "- Ada: simple\n",
    "- Gradient: moderate\n",
    "- XG: high\n",
    "\n",
    "**Overfitting:**\n",
    "- Ada: Prone on noisy data\n",
    "- Gradient: Less prone (depends on parameters)\n",
    "- XG: less prone due to regularization\n",
    "\n",
    "**Handling Large Data:**\n",
    "- Ada: moderate\n",
    "- Gradient: moderate\n",
    "- XG: Excellent (optimized for large-scale data)\n",
    "\n",
    "**Tuning Parameters:**\n",
    "- Ada: few parameters\n",
    "- Gradient: more parameters\n",
    "- XG: many parameters\n",
    "\n",
    "## Stacking (Stacked Generalization)\n",
    "- Training multiple different models (level-0 models/base learner)\n",
    "- Use another model (meta-learner) to aggregate predictions\n",
    "- Meta-learner is trained on the output of the base models\n",
    "\n",
    "### Stacking Features\n",
    "- Model diversity - leveraging individual model's strength\n",
    "- Meta-learning - learns hwo to best combine the base model's predictions\n",
    "- Use cross-validation to generate the training data for the meta-learner\n",
    "- No specific algorithm: can use any combo of modes as base learners and any model as a meta-learner\n",
    "\n",
    "### Advantages\n",
    "- High flexibility\n",
    "- Better performance\n",
    "- Customizability\n",
    "\n",
    "### Disadvantages\n",
    "- Complexity: complex to implement and tune\n",
    "- Computational cost: training multiple models and meta-learner\n",
    "- Risk of overfitting: need properly validate the model\n",
    "\n",
    "## When to Use Each Boosting\n",
    "- AdaBoost: simple problem, need a quick, easy-to-implement solution, be cautious with noisy data\n",
    "- Gradient Boosting: need a flexible and powerful model that can handle various type of data and loss function. be prepared for longer training time and more parameter tuning\n",
    "- XGBoost: need high performance and efficiency, especially for large dataset. Ideal for competitive machine learning tasks.\n",
    "\n",
    "## When to Use Each Ensemble\n",
    "- Bagging: want to reduce variance and improve stability, especially with high variance models (decision trees), suitable for parallel processing\n",
    "- Boosting: need to improve accuracy by reducing bias and variance. Ideal for challenging datasets and problems where performance is critical\n",
    "- Stacking: when you have diverse models. Suitable for complex tasks that can afford computational complexity\n",
    "\n",
    "# XG Boost\n",
    "\n",
    "### What is it?\n",
    "- Optimized distributed gradient boosting library\n",
    "- Focus on efficiency and model performance\n",
    "- Include several enhancements: regularization, tree pruning, parallel processing\n",
    "\n",
    "### Benefit (Optimization + Efficiency)\n",
    "- Regularization (L1 & L2) -> prevent overfitting\n",
    "- Parallel processing -> efficiency with parallel tree construction\n",
    "- Handling missing values\n",
    "- Tree pruning -> advanced pruning based on max depth\n",
    "- High flexibility and portability\n",
    "- Faster training time and efficient prediction\n",
    "\n",
    "### Disadvantages\n",
    "- Complexity in both understanding and implementation\n",
    "- Resource intensive - requiring more memory and processing power"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
