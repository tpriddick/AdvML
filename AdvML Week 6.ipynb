{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional NLP\n",
    "\n",
    "- Part of AI, intersection of computer science, artificial intelligence, and linguistics\n",
    "- Concerns with the interactions between computer and natural language (human language)\n",
    "- Understand and process human languages\n",
    "- Deal with unstructured textual data: text, video, images\n",
    "\n",
    "## Tasks\n",
    "- Text classification\n",
    "- Sentiment analysis\n",
    "- Named entity recognition\n",
    "- Machine translate\n",
    "- Text summary\n",
    "\n",
    "## NLP Operations\n",
    "- Tokenization: splitting text into individual words or sentence\n",
    "- Stopword removal: removing common words that do not contain significant meaning\n",
    "- Stemming and lemmatization: reducing words to their base or root forms\n",
    "- Part-of-speech (PoS) tagging: assigning parts of speech to each word (noun, verb, etc.)\n",
    "- Named Entity Recognition (NER): identify adn classify named entities (name, dates)\n",
    "- Bag-of-words (BoW): representing text as a collection of words frequencies\n",
    "- TF-IDF (term frequency-inverse document frequency): a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents\n",
    "\n",
    "### TF-IDF\n",
    "- Term frequency: how important is the word in the document?\n",
    "    - Term frequency is the number of occurances of a given word in document\n",
    "- Inverse document frequency: how important is that term in the whole corpus?\n",
    "    - log(number of documents in the corpus/number of documents that include that word)\n",
    "\n",
    "# NLP Libraries in Python\n",
    "- NLTK (Natural Language Toolkit): a comprehensive library for various NLP tasks\n",
    "- spaCy: an industrial-strength NLP library with efficient and easy-to-use features\n",
    "- TextBlob: a simple library for processing textual data\n",
    "- Scikit-learn: text classification and other NLP tasks\n",
    "- Transformers (by Hugging Face): state-of-the-art NLP with transformer models\n",
    "\n",
    "# NLTK Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') # pretrained parts of speech for English\n",
    "nltk.download('punkt') # pretrained english tokenizer\n",
    "nltk.download('stopwords') # list of 179 English stopwords\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone, this lecture is about natural language processing.', 'It is week 6 already']\n"
     ]
    }
   ],
   "source": [
    "# tokenization -- paragraph to sentences:\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello everyone, this lecture is about natural language processing. It is week 6 already\"\n",
    "results = sent_tokenize(text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', ',', 'this', 'lecture', 'is', 'about', 'natural', 'language', 'processing', '.', 'It', 'is', 'week', '6', 'already']\n"
     ]
    }
   ],
   "source": [
    "results = word_tokenize(text)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello : hello\n",
      "everyone : everyon\n",
      ", : ,\n",
      "this : thi\n",
      "lecture : lectur\n",
      "is : is\n",
      "about : about\n",
      "natural : natur\n",
      "language : languag\n",
      "processing : process\n",
      ". : .\n",
      "It : it\n",
      "is : is\n",
      "week : week\n",
      "6 : 6\n",
      "already : alreadi\n"
     ]
    }
   ],
   "source": [
    "# Porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words = results\n",
    "for w in words:\n",
    "    print(w, \":\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancaster Stemmer and Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello : Hello\n",
      "everyone : everyone\n",
      ", : ,\n",
      "this : this\n",
      "lecture : lecture\n",
      "is : is\n",
      "about : about\n",
      "natural : natural\n",
      "language : language\n",
      "processing : processing\n",
      ". : .\n",
      "It : It\n",
      "is : is\n",
      "week : week\n",
      "6 : 6\n",
      "already : already\n"
     ]
    }
   ],
   "source": [
    "# Wordnet Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    print(w, \":\", lemma.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', ',', 'this', 'lecture', 'is', 'about', 'natural', 'language', 'processing', '.', 'It', 'is', 'week', '6', 'already']\n",
      "['Hello', 'everyone', ',', 'lecture', 'natural', 'language', 'processing', '.', 'It', 'week', '6', 'already']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "['Hello', 'everyone', ',', 'lecture', 'natural', 'language', 'processing', '.', 'It', 'week', '6', 'already']\n",
      "\n",
      " PoS tagging results: \n",
      "Hello : NNP\n",
      "everyone : NN\n",
      ", : ,\n",
      "lecture : JJ\n",
      "natural : JJ\n",
      "language : NN\n",
      "processing : NN\n",
      ". : .\n",
      "It : PRP\n",
      "week : NN\n",
      "6 : CD\n",
      "already : RB\n"
     ]
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "# Refer to NLTK PoS tags for clarification\n",
    "pos_tags = pos_tag(filtered_sentence)\n",
    "\n",
    "print('Original text: ')\n",
    "print(filtered_sentence)\n",
    "print('\\n PoS tagging results: ')\n",
    "\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"{word} : {pos_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hello/NNP)\n",
      "  all/DT\n",
      "  ,/,\n",
      "  (PERSON Jameson/NNP)\n",
      "  is/VBZ\n",
      "  giving/VBG\n",
      "  a/DT\n",
      "  (ORGANIZATION NLP/NNP)\n",
      "  lecture/NN\n",
      "  today/NN\n",
      "  ./.\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  meet/VB\n",
      "  in/IN\n",
      "  Ford/NNP\n",
      "  201/CD\n",
      "  this/DT\n",
      "  Friday/NNP\n",
      "  evening/NN\n",
      "  from/IN\n",
      "  6-10/JJ\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "text = \"Hello all, Jameson is giving a NLP lecture today. We will meet in Ford 201 this Friday evening from 6-10.\"\n",
    "token = word_tokenize(text)\n",
    "postag = nltk.pos_tag(token)\n",
    "ner = nltk.ne_chunk(postag, binary = False)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity score for s2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/leotyler/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.773, 'neu': 0.227, 'pos': 0.0, 'compound': -0.5267}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "s2 = \"That's disgusting\"\n",
    "print('polarity score for s2: ')\n",
    "sia.polarity_scores(s2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
